# Notre Dame Digitized Latin Collection: Tools

**Authors**:
- Stephen Bothwell, Kaitlin Stephan, Hildegund MÃ¼ller, and David Chiang (University of Notre Dame)

**Maintainer**: Stephen Bothwell

## Summary

This repository contains code and supplementary data related to the authors' work on two datasets: the [Notre Dame Digitized Latin Collection (ND-DLC)](https://github.com/Mythologos/Notre-Dame-Digitized-Latin-Collection) and the [Corpus Correctum (Cor<sup>2</sup>)](). 
The codebase includes a variety of command line interfaces (CLIs) which can permit the process of OCR-ing and post-correcting texts. 
In particular, these tools are heavily focused on Latin texts.

### Use

...

## Directory Structure

This codebase contains three major fixtures. The first is (as previously mentioned) a set of command line interfaces, all of which are in the main directory. 
The second is a set of utility functions which are used to organize and drive the functionality of these command line interfaces; these are presented in the `utils` subdirectory. 
The third is a set of supplementary files which the command line interfaces use or produce; these are presented in the `data` directory.

In the following subsections, we will discuss the code and the supplementary data in turn.

### Command-Line Interfaces

For each CLI, we will provide the help message generated by Python's `argparse` and will give clarifying information and comments as needed.
To install packages to use these interfaces, please refer to the `prosody.yml` file. 
This file contains all required packages and versions for using the CLIs.
We also provide the separate `lace_builder.yml` file to use that codebase for packaging images and hOCR files for use in Lace.

#### Preparing a PDF for OCR (`pdf_converter.py`)

To start the process of creating a digital edition, we first needed to find scans for the pages of a physical edition. 
These scans are often stored in PDFs rather than as independent image. 
We apply the `pdf2image` library to do this. We provide a number of the command line options that that library does to facilitate splitting PDFs under different conditions. 
The main optional flag in use was grayscaling, and we only applied it when the pages were in color. 

Regarding saving extracted pages, the `pdf2image` library saves each image with a four-digit text ID (starting from `0001`) and a four-digit page ID (starting from the same point). 
The default `output-format` will save pages as `page-XXXX-YYYY`, where `XXXX` is the text ID and `YYYY` is the page ID.

```
>>> python pdf_converter.py -h
usage: pdf_converter.py [-h] [--cropbox | --no-cropbox] [--dpi DPI] [--grayscale | --no-grayscale] --input-path INPUT_PATH --output-directory OUTPUT_DIRECTORY
                        [--output-format OUTPUT_FORMAT] [--output-prefix OUTPUT_PREFIX] [--pdftocairo | --no-pdftocairo] [--strict-errors | --no-strict-errors]

options:
  -h, --help            show this help message and exit
  --cropbox, --no-cropbox
                        flag indicating whether to apply cropbox instead of mediabox (default: False)
  --dpi DPI             dots per inch corresponding to PDF input
  --grayscale, --no-grayscale
                        flag indicating whether to apply grayscale onto segmented images (default: False)
  --input-path INPUT_PATH
                        path to input PDF file or directory containing PDF files
  --output-directory OUTPUT_DIRECTORY
                        path to directory in which output images will be saved
  --output-format OUTPUT_FORMAT
                        format of output images
  --output-prefix OUTPUT_PREFIX
                        filename prefix for output images
  --pdftocairo, --no-pdftocairo
                        flag indicating whether to apply Cairo instead of a Portable Pixmap (ppm) (default: False)
  --strict-errors, --no-strict-errors
                        flag indicator whether to throw exceptions on PDF syntax errors (default: False)
```

**Note**: If you are using Windows, be sure to install the Poppler library! 
The `pdf2image` library provides instructions on how to do so in [its README](https://github.com/Belval/pdf2image).

#### Applying Tesseract OCR (`hocr_applier.py`)

After we obtain discrete pages from a PDF, we can apply an OCR model from Tesseract to these changes with the `pytesseract` library. 
We used the Latin (`lat`) and Ancient Greek (`grc`) models from Tesseract for all texts. 
We save the result for each page as a hOCR file. 
When combined with the image files produced by `pdf_converter.py` and some XML-based metadata files, this data can be used with the Lace OCR post-correction software.

```
>>> python hocr_applier.py -h   
usage: hocr_applier.py [-h] [--input-format INPUT_FORMAT] --input-path INPUT_PATH [--languages [LANGUAGES ...]] --output-directory OUTPUT_DIRECTORY

options:
  -h, --help            show this help message and exit
  --input-format INPUT_FORMAT
                        type of image to be supplied to Tesseract
  --input-path INPUT_PATH
                        path to image file or directory containing image files upon which Tesseract will be applied
  --languages [LANGUAGES ...]
                        the codes corresponding to the languages for which OCR models will be applied
  --output-directory OUTPUT_DIRECTORY
                        path to directory where hOCR files resulting from the use of Tesseract will be saved
```

To package the PDF images and Tesseract hOCR files for Lace, we used the `lacebuilder` tool (Robertson, 2024). 
Note that this tool requires dependencies whose versions may conflict with our other environment. 
Thus, we have the separate `lace.yml` file for setting up and using this tool.

Sample commands applied with `lacebuilder` include one for packing images:

```
lacebuilder --outputdir data/ocr/xar/csel --metadatafile data/ocr/metadata/csel/32_1.xml packimages --imagedir data/ocr/images/csel
```

... and one for packing images with their corresponding hOCR files:

```
lacebuilder --outputdir data/ocr/xar/csel --metadatafile data/ocr/metadata/csel/32_1.xml packtexts --imagexarfile data/ocr/xar/csel/csel-32-1_images.xar --hocrdir data/ocr/hocr/csel/32-1 --ocr-engine tesseract --classifier lat+grc
```

#### Collecting Materials for a Spellchecker (`wiktionary_scraper.py`, `wiktionary_parser.py`, `wordlist_constructor.py`)

After post-correcting Tesseract's OCR for our texts in Lace, we applied a set of additional post-correction steps. 
One such step involved using a Latin spellchecker. To build this spellchecker, we needed to (1) gather lists of valid Latin words, and (2) determine their frequency. 
Having these values together would create a mapping that could be integrated with the `pyspellchecker` library.

One wordlist we chose to gather was from Wiktionary. 
Wiktionary documents a variety of Latin words across its long history, making it an ideal resource for our multi-period set of digital editions. 
We used the `scrapy` library to gather pages tagged under the ["Latin lemmas" category](https://en.wiktionary.org/wiki/Category:Latin_lemmas) (Wiktionary Contributors, 2022; Zyte, 2021). 
We filtered out pages whose headwords met certain qualifications. For instance:
- If a headword was an *affix* (*i.e.*, a prefix like "prae-" or a suffix like "-mentum"), we excluded it.
- If a headword contained non-alphabetic characters, we excluded it.
- If a headword contained multiple words (detected via the presence of space characters), we excluded it.
- If a headword was reconstructed--in other words, only theoretically representing a Latin word in practice--we excluded it. These are generally indicated with an asterisk (*e.g.*, the entry "*nivare", perhaps meaning "to snow").

We applied all filters; however, the option is provided not to use some (or any) filters. 
Use of this tool will take some time due to all the pages that need to be scraped. 
In addition, due to the changing nature of Wiktionary, subsequent scrapes will likely not be identical. 
All pages are saved as HTML files.

```
>>> python wiktionary_scraper.py -h
usage: wiktionary_scraper.py [-h] [--filters {affix,non-alphabetic,multiword,reconstruction} [{affix,non-alphabetic,multiword,reconstruction} ...]] --input-filepath
                             INPUT_FILEPATH --output-directory OUTPUT_DIRECTORY

options:
  -h, --help            show this help message and exit
  --filters {affix,non-alphabetic,multiword,reconstruction} [{affix,non-alphabetic,multiword,reconstruction} ...]
                        the names of functions to apply to filter out Wiktionary page entries
  --input-filepath INPUT_FILEPATH
                        path to file containing one or more URLs from which to extract Latin Wiktionary pages
  --output-directory OUTPUT_DIRECTORY
                        path to directory in which scraped Latin Wiktionary pages will be saved
```

Once we had those HTML files, we then applied a second tool to parse these pages for their Latin headwords and inflection tables. 
We used `bs4`, or Beautiful Soup 4, to perform this parsing.
For each page, we created a text file containing all headwords and their inflections on independent lines.

```
>>> python wiktionary_parser.py -h  
usage: wiktionary_parser.py [-h] --input-directory INPUT_DIRECTORY --output-directory OUTPUT_DIRECTORY

options:
  -h, --help            show this help message and exit
  --input-directory INPUT_DIRECTORY
                        path to directory in which Latin Wiktionary pages are saved
  --output-directory OUTPUT_DIRECTORY
                        path to directory in which parses of Latin Wiktionary pages for inflections are saved
```

With that step completed, we then used one more command line tool to read through the per-page lists created and to create one complete wordlist (via the `wiktionary` argument to `--wordlist`). 
This tool also supports the creation of three other wordlists. 
The `lewis-short` list is from the Perseus Digital Library's (Smith, Rydberg-Cox, and Crane, 2000) digital edition of *The Latin Dictionary* by Lewis and Short (1879).
The `keeline-kirby` list derives from a list of macronized forms in Latin; this list was composed for computational tools that assist in the study of metrical patterns in Latin (Keeline and Kirby, 2019).
The `vulgate` list extracts a set of proper names from the version of Jerome's *Vulgate* available in the Tesserae project (Coffee *et al.*, 2013). 
This text was, in turn, taken from the Perseus Digital Library (Smith, Rydberg-Cox, and Crane, 2000). Their original source, made available by the Bible Foundation and On-Line Book Initiative, is no longer online (see the Jerome citation below).

```
>>> python wordlist_constructor.py -h
usage: wordlist_constructor.py [-h] --wordlist {lewis-short,keeline-kirby,vulgate,wiktionary}

options:
  -h, --help            show this help message and exit
  --wordlist {lewis-short,keeline-kirby,vulgate,wiktionary}
                        the name of the wordlist to build; wordlist data is loaded and saved automatically at preset paths
```

#### Constructing a Latin Spellchecker (`spelling_map_builder.py`)

With a set of wordlists, we have a basis for a Latin spellchecker. 
Predicting the most likely candidates for a correction requires additional information, however.
The `pyspellchecker` library (Barrus, 2024), which we use to construct our own spellchecker for Latin, uses word frequency.
We assume that each word gathered from any of our wordlists has a default frequency of 1.
Then, we use the largest Latin corpora that we are aware of to build a word-to-frequency map. 
This tool builds a JSON file that contains this mapping.

We provide two corpora as options: the Latin BERT corpus (Bamman and Burns, 2020) and the *Corpus Corporum* corpus (Roelli, 2014). 
As indicated in the `data` directory, while the former is publicly available, the latter was obtained upon request to Philipp Roelli.

```
>>> python spelling_map_builder.py -h 
usage: spelling_map_builder.py [-h] [--corpora {corpus-corporum,latin-bert-corpus} [{corpus-corporum,latin-bert-corpus} ...]] --output-filepath OUTPUT_FILEPATH
                               [--wordlists {lewis-short,keeline-kirby,vulgate,wiktionary} [{lewis-short,keeline-kirby,vulgate,wiktionary} ...]]

options:
  -h, --help            show this help message and exit
  --corpora {corpus-corporum,latin-bert-corpus} [{corpus-corporum,latin-bert-corpus} ...]
                        the names of corpora to use to compute frequencies for Latin words
  --output-filepath OUTPUT_FILEPATH
                        path to JSON file in which the map between Latin words and their frequencies is saved
  --wordlists {lewis-short,keeline-kirby,vulgate,wiktionary} [{lewis-short,keeline-kirby,vulgate,wiktionary} ...]
                        the names of wordlists to use to form the vocabulary for the spellchecker
```

#### Using a Latin Spellchecker (`spelling_checker.py`)

At last, once we have a word-to-frequency map, we can apply the Latin spellchecker. 
Along with this map, this spellchecker takes as input a text file. 
For each text to which we applied the spellchecker, we created a text file with a sentence on each line.
In its output file, the spellchecker reports which words in a line are not known and which line they are on. 

This spellchecker contains some Latin-specific handling. 
For instance, it avoids flagging valid Latin words which have valid Latin enclitics (*e.g.*, "-que", "-ve") attached to them.

This tool is not perfect, as its wordlists certainly do not and cannot cover the whole of Latin. 
Many proper nouns will be flagged. 
In addition, syncopated forms may also not have appeared in any of our wordlists. 
These and other issues could be rectified to improve the spellchecker.

```
>>> python spelling_checker.py -h     
usage: spelling_checker.py [-h] --map-filepath MAP_FILEPATH [--minimum-distance MINIMUM_DISTANCE] --input-filepath INPUT_FILEPATH --output-filepath OUTPUT_FILEPATH

options:
  -h, --help            show this help message and exit
  --map-filepath MAP_FILEPATH
                        path to JSON file in which the map between Latin words and their frequencies is saved
  --minimum-distance MINIMUM_DISTANCE
                        the minimum Levenshtein distance used to indicate spelling errors
  --input-filepath INPUT_FILEPATH
                        path to a text file with a line per text unit (e.g., sentence) which will be spellchecked
  --output-filepath OUTPUT_FILEPATH
                        path to a (potentially nonexistent) text file which will contain spellchecking results
```

#### Analyzing a Digital Edition (`epidoc_analyzer.py`)

Our next CLI computes statistics for our digital editions. 
Given a directory containing XML files, this program retrieves and loads the text from these files. 
After some minor preprocessing, it uses the `LatinWordTokenizer` and `LatinPunktSentenceTokenizer` from the Classical Language Toolkit (CLTK) to split and count the words and sentences in each file (Johnson *et al.*, 2021). 
Note that, for our word count, we do *not* count enclitics separately and do not count punctuation marks at all.
Results are printed to the console, and they are listed by author and title.

```
>>> python epidoc_analyzer.py -h
usage: epidoc_analyzer.py [-h] --input-directory INPUT_DIRECTORY

options:
  -h, --help            show this help message and exit
  --input-directory INPUT_DIRECTORY
                        path to directory containing EpiDoc-compliant digital editions of texts
```

#### Cleaning OCR and Post-OCR Data (`lace_ocr_training_generator.py`, `lace_training_checker.py`, `lace_training_postprocessor.py`)

In addition to our scripts for creating digital editions, we also present additional scripts for developing OCR and post-OCR datasets.
After downloading OCR and post-OCR datasets from Lace, we employ a few scripts to clean and reformat these datasets. 

Our first script takes in a TSV file from Lace. 
It then cleans a few common issues: 
bookmark characters (ðŸ“–Ã—) left in the text,
directional quotation marks facing the wrong direction,
and XML artifacts (*e.g.*, `&amp;`) being retained in the text.
It also allows for the absolute filepaths that Lace traditionally uses to be reformatted into relative filepaths.

```
>>> python lace_training_postprocessor.py -h 
usage: lace_training_postprocessor.py [-h] --current-tsv-filepath CURRENT_TSV_FILEPATH --dataset-type DATASET_TYPE [--filter-bookmarks | --no-filter-bookmarks]
                                      [--filter-xml-artifacts | --no-filter-xml-artifacts] [--fix-smart-quotes | --no-fix-smart-quotes]
                                      [--new-image-directory NEW_IMAGE_DIRECTORY] --new-tsv-filepath NEW_TSV_FILEPATH

options:
  -h, --help            show this help message and exit
  --current-tsv-filepath CURRENT_TSV_FILEPATH
                        path to where TSV file to be postprocessed is stored
  --dataset-type DATASET_TYPE
                        the type of dataset (e.g., OCR or post-OCR) to be processed
  --filter-bookmarks, --no-filter-bookmarks
                        flag to determine whether bookmark symbols will be cleaned (default: False)
  --filter-xml-artifacts, --no-filter-xml-artifacts
                        flag to determine whether XML artifacts (e.g., &amp;) will be cleaned (default: False)
  --fix-smart-quotes, --no-fix-smart-quotes
                        flag to determine whether initial smart quotes will be fixed to be the correct character (default: False)
  --new-image-directory NEW_IMAGE_DIRECTORY
                        path to directory where images will be relocated
  --new-tsv-filepath NEW_TSV_FILEPATH
                        path to where TSV file with postprocessing changes will be stored
```

While many errors can be handled automatically, some are less straightforward. 
The current version of Lace is beholden to the structure of hOCR files: 
if the same text is in multiple bounding boxes, it may be represented more than once (depending on transcription policy).
As a result, this script helps to find potential adjacent duplicates so that they can be manually remedied.

``` 
python lace_training_checker.py -h       
usage: lace_training_checker.py [-h] [--output-filepath OUTPUT_FILEPATH] --tsv-filepath TSV_FILEPATH

options:
  -h, --help            show this help message and exit
  --output-filepath OUTPUT_FILEPATH
                        path to file where lines with potential text duplication will be recorded
  --tsv-filepath TSV_FILEPATH
                        a path to a TSV file containing OCR data in Lace's designated format
```

The final script we present for dataset cleaning and formatting takes a Lace OCR file as input. 
It saves the resulting TSV file as a set of cropped images and ground truth transcription text files. 
In this format, the data becomes suitable for use in training models with OCR engines like Tesseract (Smith, 2007) and Calamari (Wick, 2020).

```
>>> python lace_ocr_training_generator.py -h
usage: lace_ocr_training_generator.py [-h] [--image-format {jpg,png,tif}] --output-directory OUTPUT_DIRECTORY --tsv-filepath TSV_FILEPATH [-v | --verbose | --no-verbose]

Generate OCR training set from Lace TSV output.

options:
  -h, --help            show this help message and exit
  --image-format {jpg,png,tif}
                        type of image to save as output alongside ground truth text transcriptions
  --output-directory OUTPUT_DIRECTORY
                        path to directory where images and ground truth text transcriptions will be stored
  --tsv-filepath TSV_FILEPATH
                        a path to a TSV file containing OCR data in Lace's designated format
  -v, --verbose, --no-verbose
                        flag indicating whether additional information about training set generation should be displayed (default: False)
```

#### Analyzing OCR and Post-OCR Data (`lace_dataset_analyzer.py`)

Lastly, we present a script to analyze OCR and post-OCR data. 
Metrics available include: page count, line count, word count, character count, and a pair of character error rates with different origin points (specifically for the post-OCR dataset). 
Note that "word" units are determined by our version of the `LatinWordTokenizer` from CLTK, 
and they may count both ends of hyphenated units due to the structure of the data.

This script prints results to the console on a per-file basis and also in aggregate. 
For character error rates, we compute a micro-average across all TSV files. 
In other words, we sum all expected maximum error counts and actual error counts before computing the error rate.

```
>>> python lace_dataset_analyzer.py -h  
usage: lace_dataset_analyzer.py [-h] --dataset-type DATASET_TYPE [--metrics METRICS [METRICS ...]] [--processes PROCESSES] --tsv-directory TSV_DIRECTORY

options:
  -h, --help            show this help message and exit
  --dataset-type DATASET_TYPE
                        the type of dataset (e.g., OCR or post-OCR) to be processed
  --metrics METRICS [METRICS ...]
                        a set of one or more metrics to be computed in dataset analysis
  --processes PROCESSES
                        number of processes to be used for computing metrics; only applies to error rates
  --tsv-directory TSV_DIRECTORY
                        path to directory where collection of TSV files are stored
```

### Supplementary Data

In this subsection, we describe the additional data that we provide in this repository to facilitate reproducing our experiments. 
We discuss the major subdirectories in our `data` directory below.

#### Latin Corpora (`data/corpora`)

The `corpora` subdirectory contains large corpora for Latin.
These corpora are used to build a word-to-frequency map for the Latin spellchecker. 
We currently provide support for using two corpora. 
The first was used to train the Latin BERT language model (Bamman and Burns, 2020). 
The second is the Corpus Corporum, a collection of a variety of different Latin textual resources converted into a common TEI standard (Roelli, 2014).

#### Spelling Map (`data/mapping`)

The `mapping` subdirectory contains the word-to-frequency map for the Latin spellchecker. This is stored in a JSON file. 
Currently, the map file (`klvw-lb-map.json`) has been named based on its component wordlists (a letter per wordlists; `k` being `keeline-kirby`, `l` being `lewis-short`, and so on) 
and its training corpus (`lb` being `latin-bert`). 

#### Stages of Digitization (`data/ocr`)

The `ocr` subdirectory contains stages of the digitization process. 
There are a set of five additional subdirectories: `hocr`, `images`, `metadata`, `pdf`, and `xar`. 
The `pdf` subdirectory contains (when possible) original PDFs used in our pipeline.
The `metadata` subdirectory holds XML files which present a set of attributes in TEI XML about the physical editions.
The `images` subdirectory is meant to include images extracted from PDFs.
The `hocr` subdirectory is meant to include hOCR files describing the results of applying Tesseract for OCR.
The `xar` subdirectory includes archival files for images, hOCR files, and metadata files which can be given as input to Lace.

#### Wordlists (`data/wordlist`)

The `wordlist` subdirectory contains information about input and output data for each of the wordlists that went into forming our vocabulary for our Latin spellchecker. 
As mentioned above, there are four sources:
- `keeline-kirby`: In their work on applying computational methods to capture Latin metrical rhythm, Keeline and Kirby (2019) created a wordlist and set up code to macronize Latin text. We reformat and make use of their wordlist.
- `lewis-short`: Charlton A. Lewis and Charles Short's *A Latin Dictionary* is a prominent resource for the study of Latin texts (Lewis and Short, 1978). We mine headwords from the Perseus Digital Library's (Smith, Rydberg-Cox, and Crane, 2000) digital edition of this text for our wordlist. We additionally use the Collatinus tool (Ouvrard and Verkerk, 2014) to inflect all headwords.
- `vulgate`: Many of our texts are from Christian authors who make use of Latinized Hebrew. To capture many of the proper names which we expected to otherwise be flagged, we mined Jerome's *Vulgate* as presented by the Tesserae project (Coffee *et al.*, 2013) for proper names by searching for capitalized words, as only proper nouns were capitalized in their text.
- `wiktionary`: As described above, we scraped Wiktionary's collection of Latin headwords (starting from their page on Latin lemmata) and inflection tables for Latin forms (Wiktionary Contributors, 2022).

In general, we only provide links to the input files. 
However, we do provide the `wiktionary` subdirectory contains the scrape of `pages` that we gathered from Wiktionary, starting at roughly 10 A.M. ET on April 10th, 2024. 
This was done to promote reproducibility, as it would not be possible with the current code to reproduce the scrape done at a specific time.
This subdirectory also provides a link to the URL where our search began in the file `base_urls.txt`.

Each of the `output` subdirectories contains a text file with the final list of words mined from the relevant resource. 
Each word of each list is on its own line.

## Contributing

The code in this repository was altered and cleaned from its original state prior to release to promote usability, but it is possible that bugs were introduced in the process (or were present beforehand). If you experience issues in using this code or would like further instructions to reproduce our procedure, please feel free to submit an issue regarding this.

We do not intend to heavily maintain this code, as it is meant to represent our paper at its time of publication. Exceptions may be made if warranted (*e.g.*, there is a bug which prevents the code from being correctly run), and we are happy to provide clarifications or assistance in reproducing our results.

## Citations

To cite this repository, please use the following citation:

```bibtex
...
```

For other libraries noted above, see the below bibliographic entries:

```bibtex
@misc{bammanLatinBERTContextual2020,
  title = {Latin {{BERT}}: {{A}} Contextual Language Model for Classical Philology},
  shorttitle = {Latin {{BERT}}},
  author = {Bamman, David and Burns, Patrick J.},
  year = {2020},
  month = sep,
  eprint = {2009.10053},
  urldate = {2020-09-27},
  abstract = {We present Latin BERT, a contextual language model for the Latin language, trained on 642.7 million words from a variety of sources spanning the Classical era to the 21st century. In a series of case studies, we illustrate the affordances of this language-specific model both for work in natural language processing for Latin and in using computational methods for traditional scholarship: we show that Latin BERT achieves a new state of the art for part-of-speech tagging on all three Universal Dependency datasets for Latin and can be used for predicting missing text (including critical emendations); we create a new dataset for assessing word sense disambiguation for Latin and demonstrate that Latin BERT outperforms static word embeddings; and we show that it can be used for semantically-informed search by querying contextual nearest neighbors. We publicly release trained models to help drive future work in this space.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
}

@misc{barrusBarrustPyspellchecker2025,
  title = {Barrust/Pyspellchecker},
  author = {Barrus, Tyler},
  urldate = {2025-02-07},
  abstract = {Pure Python Spell Checking http://pyspellchecker.readthedocs.io/en/latest/},
  copyright = {MIT},
  keywords = {levenshtein-distance,python,python-spell-checking,spellcheck,spellchecker,spelling-checker},
  version = {0.8.1}
}

@misc{belvalBelvalPdf2image2025,
  title = {Belval/Pdf2image},
  author = {Belval, Edouard},
  urldate = {2025-01-09},
  abstract = {A python module that wraps the pdftoppm utility to convert PDF to PIL Image object},
  copyright = {MIT},
  keywords = {convert,pdf,pil,pil-image,poppler},
  version = {1.17.0}  
}

@article{coffeeTesseraeProjectIntertextual2013,
  title = {The {{Tesserae Project}}: Intertextual Analysis of {{Latin}} Poetry},
  author = {Coffee, Neil and Koenig, Jean-Pierre and Poornima, Shakthi and Forstall, Christopher W. and Ossewaarde, Roelant and Jacobson, Sarah L.},
  year = {2013},
  month = jun,
  journal = {Literary and Linguistic Computing},
  volume = {28},
  number = {2},
  pages = {221--228},
  issn = {0268-1145},
  doi = {10.1093/llc/fqs033},
  urldate = {2025-02-07},
  abstract = {Tesserae is a web-based tool for automatically detecting allusions in Latin poetry. Although still in the start-up phase, it already is capable of identifying significant numbers of known allusions, as well as similar numbers of allusions previously unnoticed by scholars. In this article, we use the tool to examine allusions to Vergil's Aeneid in the first book of Lucan's Civil War. Approximately 3,000 linguistic parallels returned by the program were compared with a list of known allusions drawn from commentaries. Each was examined individually and graded for its literary significance, in order to benchmark the program's performance. All allusions from the program and commentaries were then pooled in order to examine broad patterns in Lucan's allusive techniques which were largely unapproachable without digital methods. Although Lucan draws relatively constantly from Vergil's generic language in order to maintain the epic idiom, this baseline is punctuated by clusters of pointed allusions, in which Lucan frequently subverts Vergil's original meaning. These clusters not only attend the most significant characters and events but also play a role in structuring scene transitions. Work is under way to incorporate the ability to match on word meaning, phrase context, as well as metrical and phonological features into future versions of the program.}
}

@misc{jeromeVulgateBible,
  title = {Vulgate {{Bible}}},
  author = {{Jerome}},
  collaborator = {{Bible Foundation and On-Line Book Initiative}},
  howpublished = {ftp.std.com/obi/Religion/Vulgate},
  annotation = {Extant URL: https://www.perseus.tufts.edu/hopper/text?doc=Perseus:text:1999.02.0060}
}

@inproceedings{johnsonClassicalLanguageToolkit2021,
  title = {The {{Classical Language Toolkit}}: {{An NLP}} Framework for Pre-Modern Languages},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: {{System}} Demonstrations},
  author = {Johnson, Kyle P. and Burns, Patrick J. and Stewart, John and Cook, Todd and Besnier, Cl{\'e}ment and Mattingly, William J. B.},
  year = {2021},
  month = aug,
  pages = {20--29},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.acl-demo.3},
  abstract = {This paper announces version 1.0 of the Classical Language Toolkit (CLTK), an NLP framework for pre-modern languages. The vast majority of NLP, its algorithms and software, is created with assumptions particular to living languages, thus neglecting certain important characteristics of largely non-spoken historical languages. Further, scholars of pre-modern languages often have different goals than those of living-language researchers. To fill this void, the CLTK adapts ideas from several leading NLP frameworks to create a novel software architecture that satisfies the unique needs of pre-modern languages and their researchers. Its centerpiece is a modular processing pipeline that balances the competing demands of algorithmic diversity with pre-configured defaults. The CLTK currently provides pipelines, including models, for almost 20 languages.}
}

@article{keelineAucepsSyllabarumDigital2019,
  title = {Auceps {{Syllabarum}}: {{A}} Digital Analysis of {{Latin}} Prose Rhythm},
  author = {Keeline, Tom and Kirby, Tyler},
  year = {2019},
  journal = {The Journal of Roman studies},
  volume = {109},
  pages = {161--204},
  publisher = {Cambridge University Press},
  address = {Cambridge, UK},
  issn = {0075-4358},
  abstract = {In this article we describe a series of computer algorithms that generate prose rhythm data for any digitised corpus of Latin texts. Using these algorithms, we present prose rhythm data for most major extant Latin prose authors from Cato the Elder through the second century a.d. Next we offer a new approach to determining the statistical significance of such data. We show that, while only some Latin authors adhere to the Ciceronian rhythmic canon, every Latin author is `rhythmical' --- they just choose different rhythms. Then we give answers to some particular questions based on our data and statistical approach, focusing on Cicero, Sallust, Tacitus and Pliny the Younger. In addition to providing comprehensive new data on Latin prose rhythm, presenting new results based on that data and confirming certain long-standing beliefs, we hope to make a contribution to a discussion of digital and statistical methodology in the study of Latin prose rhythm and in Classics more generally. The Supplementary Material available online (https://doi.org/10.1017/S0075435819000881) contains an appendix with tables, data and code. This appendix constitutes a static `version of record' for the data presented in this article, but we expect to continue to update our code and data; updates can be found in the repository of the Classical Language Toolkit (https://github.com/cltk/cltk).},
  copyright = {Copyright {\copyright} The Author(s) 2019. Published by The Society for the Promotion of Roman Studies},
  langid = {english},
  keywords = {Algorithms,Articles}
}

@misc{leeMadmazePytesseract2025,
  title = {Madmaze/Pytesseract},
  author = {Lee, Matthias A.},
  urldate = {2025-02-07},
  abstract = {A Python wrapper for Google Tesseract},
  copyright = {Apache-2.0},
  version = {0.3.13}
}

@book{lewisLatinDictionaryFounded1879,
  title = {A {{Latin}} Dictionary Founded on {{Andrews}}' Edition of {{Freund}}'s {{Latin}} Dictionary.},
  author = {Lewis, Charlton T. and Short, Charles},
  year = {1879},
  edition = {Rev., enl., and in great part rewritten / by Charlton T. Lewis..},
  publisher = {Clarendon Press; Oxford University Press},
  address = {Oxford [England] : New York [NY]},
  isbn = {0-19-864201-6},
  langid = {english},
  keywords = {Latin language -- Dictionaries}
}

@article{ouvrardCollatinusOutilPolymorphe2014,
  title = {Collatinus, Un Outil Polymorphe Pour l'{\'e}tude Du Latin},
  author = {Ouvrard, Yves and Verkerk, Philippe},
  year = {2014},
  journal = {Archivum Latinitatis Medii Aevi},
  volume = {72},
  number = {1},
  pages = {305--311},
  doi = {10.3406/alma.2014.1156},
  urldate = {2023-10-07},
  langid = {english}
}

@misc{richardsonBeautifulsoup4ScreenscrapingLibrary,
  title = {Beautifulsoup4: {{Screen-scraping}} Library},
  shorttitle = {Beautifulsoup4},
  author = {Richardson, Leonard},
  urldate = {2025-04-09},
  copyright = {OSI Approved :: MIT License},
  keywords = {HTML,parse,Software Development - Libraries - Python Modules,soup,Text Processing - Markup - HTML,Text Processing - Markup - SGML,Text Processing - Markup - XML,XML},
}

@article{roelliCorpusCorporumNew2014,
  title = {The {{Corpus Corporum}}, a New Open {{Latin}} Text Repository and Tool},
  author = {Roelli, Philipp},
  year = {2014},
  journal = {Archivum Latinitatis Medii Aevi},
  volume = {72},
  number = {1},
  pages = {289--304},
  publisher = {Pers{\'e}e - Portail des revues scientifiques en SHS},
  doi = {10.3406/alma.2014.1155},
  urldate = {2024-02-12},
  abstract = {Dieser Artikel pr{\"a}sentiert eine neue Online-Ressource, das Corpus Corporum, eine frei zug{\"a}ngliche lateinische Textsammlung. Es ist als Metacorpus organisiert, dessen Corpora, die aus verschiedenen Quellen stammen, einzeln oder gesamthaft studiert werden k{\"o}nnen. Die Plattform enth{\"a}lt momentan rund 130 Millionen W{\"o}rter aus allen Epochen, in denen Latein geschrieben wurde. Vielf{\"a}ltige Suchfunktionen werden angeboten, z. B. Umgebungssuchen, zeitlich eingeschr{\"a}nkte Abfragen oder lemmatisierte Suchen. Zudem k{\"o}nnen Listen von Worth{\"a}ufigkeiten und Konkordanzen einzelner W{\"o}rter generiert werden. Diese Funktionen k{\"o}nnen auf allen Ebenen, vom einzelnen Text bis zur gesamten Sammlung, ausgef{\"u}hrt werden. Das Tool ist auch n{\"u}tzlich zur schnellen Onlinelekt{\"u}re, da W{\"o}rter durch Anklicken in ihrer grammatischen Form bestimmt werden und in wichtigen W{\"o}rterb{\"u}chern angezeigt werden. Standardisierte TEI xml Dateien werden in die Datenbank eingespeist ; diese und andere Formate k{\"o}nnen vom Benutzer frei heruntergeladen werden (wenn ihre Eigent{\"u}mer damit einverstanden sind). Neben lateinischen Texten wurden versuchsweise einige griechische Texte und eine Version der hebr{\"a}ischen Bibel in die Sammlung aufgenommen. Der Artikel erkl{\"a}rt die M{\"o}glichkeiten und die Funktionsweise dieser neuen Ressource, was f{\"u}r Texte sie momentan enth{\"a}lt (mit Graphiken Textmenge pro Zeit) und endet mit einigen exemplarischen Suchen.},
  copyright = {free},
  langid = {english}
}

@misc{robertsonBrobertsonLace22024,
  title = {Brobertson/{{Lace2}}},
  author = {Robertson, Bruce},
  urldate = {2024-08-22},
  abstract = {In-broswer OCR editing program that transforms OCR results into structured, citable TEI. No XML experience required!},
  copyright = {GPL-3.0},
  keywords = {exist-db,ocr,tei-xml},
  version = {0.6.22}  
}

@misc{robertsonBrobertsonLacebuilder2024,
  title = {Brobertson/Lacebuilder},
  author = {Robertson, Bruce},
  year = {2024},
  month = may,
  urldate = {2025-04-14}
}

@article{smithPerseusProjectDigital2000,
  title = {The {{Perseus}} Project: A Digital Library for the Humanities},
  author = {Smith, David A. and {Rydberg-Cox}, Jeffrey A. and Crane, Gregory R.},
  year = {2000},
  journal = {Literary and linguistic computing},
  volume = {15},
  number = {1},
  pages = {15--25},
  publisher = {Oxford University Press},
  address = {Oxford},
  issn = {0268-1145},
  copyright = {2001 INIST-CNRS},
  langid = {english},
  keywords = {Applied linguistics,Computational linguistics,Linguistics},
}

@inproceedings{smithOverviewTesseractOCR2007,
  title = {An Overview of the Tesseract {{OCR}} Engine},
  booktitle = {Ninth International Conference on Document Analysis and Recognition ({{ICDAR}} 2007)},
  author = {Smith, R.},
  year = {2007},
  volume = {2},
  pages = {629--633},
  publisher = {IEEE},
  address = {Curitiba, Brazil},
  doi = {10.1109/ICDAR.2007.4376991},
  abstract = {The Tesseract OCR engine, as was the HP Research Prototype in the UNLV Fourth Annual Test of OCR Accuracy, is described in a comprehensive overview. Emphasis is placed on aspects that are novel or at least unusual in an OCR engine, including in particular the line finding, features/classification methods, and the adaptive classifier.},
  langid = {english},
  keywords = {Filters,Independent component analysis,Inspection,Open source software,Optical character recognition software,Pipelines,Prototypes,Search engines,Testing,Text recognition}
}

@article{wickCalamariHighPerformanceTensorflowbased2020,
  title = {Calamari - {{A High-Performance Tensorflow-based Deep Learning Package}} for {{Optical Character Recognition}}},
  author = {Wick, Christoph and Reul, Christian and Puppe, Frank},
  year = {2020},
  journal = {Digital Humanities Quarterly},
  volume = {14},
  number = {2},
  issn = {1938-4122},
  urldate = {2025-05-07},
  abstract = {Optical Character Recognition (OCR) on contemporary and historical data is still in the focus of many researchers. Especially historical prints require book specific trained OCR models to achieve applicable results [Springmann and L{\"u}deling 2017] [Reul et al. 2017a]. To reduce the human effort for manually annotating ground truth (GT) various techniques such as voting and pretraining have shown to be very efficient [Reul et al. 2018a] [Reul et al. 2018b]. Calamari is a new open source OCR line recognition software that both uses state-of-the art Deep Neural Networks (DNNs) implemented in Tensorflow and giving native support for techniques such as pretraining and voting. The customizable network architectures constructed of Convolutional Neural Networks (CNNS) and Long-Short-Term-Memory (LSTM) layers are trained by the so-called Connectionist Temporal Classification (CTC) algorithm of Graves et al. (2006). Optional usage of a GPU drastically reduces the computation times for both training and prediction. We use two different datasets to compare the performance of Calamari to OCRopy, OCRopus3, and Tesseract 4. Calamari reaches a Character Error Rate (CER) of 0.11\% on the UW3 dataset written in modern English and 0.18\% on the DTA19 dataset written in German Fraktur, which considerably outperforms the results of the existing softwares.},
  copyright = {Creative Commons Attribution-NoDerivatives 4.0 International License},
}

@misc{wiktionarycontributorsCategoryLatinLemmas2022,
  title = {Category:{{Latin}} Lemmas},
  shorttitle = {Category},
  author = {{Wiktionary contributors}},
  year = {2022},
  month = jun,
  journal = {Wiktionary, the free dictionary},
  urldate = {2024-04-06},
  abstract = {Latin lemmas, categorized by their part of speech. Category:Latin adjectives: Latin terms that give attributes to nouns, extending their definitions. Category:Latin adverbs: Latin terms that modify clauses, sentences and phrases directly. Category:Latin conjunctions: Latin terms that connect words, phrases or clauses together. Category:Latin determiners: Latin terms that narrow down, within the conversational context, the referent of the following noun. Category:Latin interjections: Latin terms that express emotions, sounds, etc. as exclamations. Category:Latin morphemes: Latin word-elements used to form full words. Category:Latin multiword terms: Latin lemmas that are a combination of multiple words, including idiomatic combinations. Category:Latin nouns: Latin terms that indicate people, beings, things, places, phenomena, qualities or ideas. Category:Latin numerals: Latin terms that quantify nouns. Category:Latin particles: Latin terms that do not belong to any of the inflected grammatical word classes, often lacking their own grammatical functions and forming other parts of speech or expressing the relationship between clauses. Category:Latin phrases: Latin groups of words elaborated to express ideas, not necessarily phrases in the grammatical sense. Category:Latin postpositions: Latin adpositions that are placed after their objects. Category:Latin prepositions: Latin adpositions that are placed before their objects. Category:Latin pronouns: Latin terms that refer to and substitute nouns. Category:Latin verbs: Latin terms that indicate actions, occurrences or states.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english}
}

@misc{zyteScrapy2021,
  title = {Scrapy},
  shorttitle = {Scrapy},
  author = {{Zyte}},
  journal = {Scrapy {\textbar} A Fast and Powerful Scraping and Web Crawling Framework},
  urldate = {2021-04-23},
  howpublished = {https://scrapy.org/},
  langid = {english},
  version = {2.11.1}  
}
```
